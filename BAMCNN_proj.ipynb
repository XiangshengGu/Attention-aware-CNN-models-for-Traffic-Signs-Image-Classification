{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b474aa1a-dcf3-47a5-a5fa-68e1f37e4631",
   "metadata": {},
   "source": [
    "# BAM + VGG/ALexNet/ResNet on Traffic Sign Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14bf972-5269-4151-b61d-3bab03f96310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import sys\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.models as models\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "# from model.attention.BAM import BAMBlock\n",
    "import numpy as np\n",
    "from torch.nn import init\n",
    "from tensorflow import keras\n",
    "sys.path.append('Documents/MLproj/G3/model/attention/BAM.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256d8d04-8912-43d0-a0db-352efc697374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produceImage(file_in, width, height, file_out):\n",
    "    image = Image.open(file_in)\n",
    "    resized_image = image.resize((width, height), Image.ANTIALIAS)\n",
    "    resized_image.save(file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ff256-3458-4c9a-a940-4bff36d48213",
   "metadata": {},
   "source": [
    "## VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bbd6912-f1b6-4725-b327-f343e57d39d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.backbone = models.vgg16(pretrained=True)\n",
    "        self.classifier = nn.Sequential(OrderedDict([\n",
    "                ('fc1',   nn.Sequential(\n",
    "                                        nn.Linear(512 * 7 * 7, 512),\n",
    "                                        nn.ReLU())),\n",
    "                ('fc2',   nn.Sequential(nn.Dropout(0.5),\n",
    "                                        nn.Linear(512, 43)\n",
    "                                        ))]))\n",
    "    def forward(self,x):\n",
    "        backbone_feat = self.backbone.features(x)\n",
    "        backbone_feat = backbone_feat.view(-1, 512 * 7 * 7)\n",
    "        score = self.classifier(backbone_feat)\n",
    "        return backbone_feat, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7b9cb10-f2d5-43e6-9a36-0d4aa7b96993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_16():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if (torch.cuda.is_available()):\n",
    "        print(f'Current device:{torch.cuda.current_device()}\\nName of device:{torch.cuda.get_device_name(0)}\\n')\n",
    "    print(\"using {} device.\".format(device))\n",
    "\n",
    "    data_transform = {\n",
    "        \"train\": transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "        \"validation\": transforms.Compose([transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}\n",
    "\n",
    "    image_path = './dataset/'  \n",
    "    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"Train\"),\n",
    "                                         transform=data_transform[\"train\"])\n",
    "    train_num = len(train_dataset)\n",
    "\n",
    "    exp_list = train_dataset.class_to_idx\n",
    "\n",
    "    batch_size = 16\n",
    "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])\n",
    "    print('Using {} dataloader workers every process'.format(nw))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=nw)\n",
    "\n",
    "    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"Test\"),\n",
    "                                            transform=data_transform[\"validation\"])\n",
    "    val_num = len(validate_dataset)\n",
    "    validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "                                                  batch_size=batch_size, shuffle=False,\n",
    "                                                  num_workers=nw)\n",
    "    print(\"using {} images for training, {} images for validation.\".format(train_num,\n",
    "                                                                           val_num))\n",
    "    \n",
    "    net = VGG16()\n",
    "    net.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()  #交叉熵损失函数\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "    epochs = 10\n",
    "    best_acc = 0.0\n",
    "    save_path = './{}Net.pth'.format('vgg')\n",
    "    train_steps = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(train_loader)\n",
    "        \n",
    "        confusion_matrix = torch.zeros((43,43),dtype=torch.int)\n",
    "        for step, data in enumerate(train_bar):\n",
    "            images, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            feat,outputs = net(images.to(device))\n",
    "\n",
    "            pred = torch.argmax(outputs.clone().detach().cpu(),dim=1)\n",
    "\n",
    "            for cur_pred, cur_label in zip(pred, labels):\n",
    "                confusion_matrix[cur_pred, cur_label] += 1\n",
    "                \n",
    "            loss = loss_function(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,\n",
    "                                                                     epochs,\n",
    "                                                                     loss)\n",
    "        # print(confusion_matrix)\n",
    "\n",
    "        # validate\n",
    "        net.eval()\n",
    "        acc = 0.0 \n",
    "        with torch.no_grad():  #验证的过程不计算损失梯度\n",
    "            val_bar = tqdm(validate_loader)\n",
    "            for val_data in val_bar:\n",
    "                val_images, val_labels = val_data\n",
    "                feat,outputs = net(val_images.to(device))\n",
    "                predict_y = torch.max(outputs, dim=1)[1]\n",
    "                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
    "\n",
    "        val_accurate = acc / val_num\n",
    "        print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
    "              (epoch + 1, running_loss / train_steps, val_accurate))\n",
    "\n",
    "        if val_accurate > best_acc:\n",
    "            best_acc = val_accurate\n",
    "            torch.save(net.state_dict(), save_path)\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b5d02f-cb67-4238-a499-3bff51e59cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device:0\n",
      "Name of device:NVIDIA TITAN RTX\n",
      "\n",
      "using cuda:0 device.\n",
      "Using 8 dataloader workers every process\n",
      "using 39209 images for training, 12630 images for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[1/10] loss:0.089: 100%|██████████| 2451/2451 [15:21<00:00,  2.66it/s]\n",
      "100%|██████████| 790/790 [01:49<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] train_loss: 0.227  val_accuracy: 0.965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[2/10] loss:0.000: 100%|██████████| 2451/2451 [15:23<00:00,  2.66it/s]\n",
      "100%|██████████| 790/790 [01:42<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] train_loss: 0.056  val_accuracy: 0.962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[3/10] loss:0.000: 100%|██████████| 2451/2451 [15:10<00:00,  2.69it/s]\n",
      "100%|██████████| 790/790 [01:42<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3] train_loss: 0.042  val_accuracy: 0.967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[4/10] loss:0.000: 100%|██████████| 2451/2451 [15:07<00:00,  2.70it/s]\n",
      "100%|██████████| 790/790 [01:42<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4] train_loss: 0.029  val_accuracy: 0.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[5/10] loss:0.000: 100%|██████████| 2451/2451 [15:13<00:00,  2.68it/s]\n",
      "100%|██████████| 790/790 [01:43<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 5] train_loss: 0.027  val_accuracy: 0.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[6/10] loss:0.000: 100%|██████████| 2451/2451 [15:14<00:00,  2.68it/s]\n",
      "100%|██████████| 790/790 [01:43<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6] train_loss: 0.023  val_accuracy: 0.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[7/10] loss:0.000: 100%|██████████| 2451/2451 [15:16<00:00,  2.67it/s]\n",
      "100%|██████████| 790/790 [01:42<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 7] train_loss: 0.022  val_accuracy: 0.963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[8/10] loss:0.000: 100%|██████████| 2451/2451 [15:20<00:00,  2.66it/s]\n",
      "100%|██████████| 790/790 [01:45<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 8] train_loss: 0.015  val_accuracy: 0.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[9/10] loss:0.000: 100%|██████████| 2451/2451 [15:24<00:00,  2.65it/s]\n",
      "100%|██████████| 790/790 [01:45<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 9] train_loss: 0.018  val_accuracy: 0.978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[10/10] loss:0.000: 100%|██████████| 2451/2451 [15:31<00:00,  2.63it/s]\n",
      "100%|██████████| 790/790 [01:45<00:00,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 10] train_loss: 0.015  val_accuracy: 0.969\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "VGG_16()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297d6113-421a-40cd-8629-f1c210f317ca",
   "metadata": {},
   "source": [
    "## Bottleneck Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de8525c2-b603-44dd-860b-4b4f96495de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return x.view(x.shape[0],-1)\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self,channel,reduction=16,num_layers=3):\n",
    "        super().__init__()\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d(1)\n",
    "        gate_channels=[channel]\n",
    "        gate_channels+=[channel//reduction]*num_layers\n",
    "        gate_channels+=[channel]\n",
    "\n",
    "\n",
    "        self.ca=nn.Sequential()\n",
    "        self.ca.add_module('flatten',Flatten())\n",
    "        for i in range(len(gate_channels)-2):\n",
    "            self.ca.add_module('fc%d'%i,nn.Linear(gate_channels[i],gate_channels[i+1]))\n",
    "            self.ca.add_module('bn%d'%i,nn.BatchNorm1d(gate_channels[i+1]))\n",
    "            self.ca.add_module('relu%d'%i,nn.ReLU())\n",
    "        self.ca.add_module('last_fc',nn.Linear(gate_channels[-2],gate_channels[-1]))\n",
    "        \n",
    "\n",
    "    def forward(self, x) :\n",
    "        res=self.avgpool(x)\n",
    "        res=self.ca(res)\n",
    "        res=res.unsqueeze(-1).unsqueeze(-1).expand_as(x)\n",
    "        return res\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self,channel,reduction=16,num_layers=3,dia_val=2):\n",
    "        super().__init__()\n",
    "        self.sa=nn.Sequential()\n",
    "        self.sa.add_module('conv_reduce1',nn.Conv2d(kernel_size=1,in_channels=channel,out_channels=channel//reduction))\n",
    "        self.sa.add_module('bn_reduce1',nn.BatchNorm2d(channel//reduction))\n",
    "        self.sa.add_module('relu_reduce1',nn.ReLU())\n",
    "        for i in range(num_layers):\n",
    "            self.sa.add_module('conv_%d'%i,nn.Conv2d(kernel_size=3,in_channels=channel//reduction,out_channels=channel//reduction,padding=1,dilation=dia_val))\n",
    "            self.sa.add_module('bn_%d'%i,nn.BatchNorm2d(channel//reduction))\n",
    "            self.sa.add_module('relu_%d'%i,nn.ReLU())\n",
    "        self.sa.add_module('last_conv',nn.Conv2d(channel//reduction,1,kernel_size=1))\n",
    "\n",
    "    def forward(self, x) :\n",
    "        res=self.sa(x)\n",
    "        res=res.expand_as(x)\n",
    "        return res\n",
    "    \n",
    "class BAMBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, channel=512,reduction=16,dia_val=2):\n",
    "        super().__init__()\n",
    "        self.ca=ChannelAttention(channel=channel,reduction=reduction)\n",
    "        self.sa=SpatialAttention(channel=channel,reduction=reduction,dia_val=dia_val)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        sa_out=self.sa(x)\n",
    "        ca_out=self.ca(x)\n",
    "        weight=self.sigmoid(sa_out+ca_out)\n",
    "        out=(1+weight)*x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b97b2ab-c8e4-4f52-a767-2a9c5a5bf9bb",
   "metadata": {},
   "source": [
    "## BAM + VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "547b2765-cc60-441b-91f1-9a9e91257464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BAM_VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BAM_VGG16, self).__init__()\n",
    "        self.backbone = models.vgg16(pretrained=True)\n",
    "        self.BAM = BAMBlock()\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.classifier = nn.Sequential(OrderedDict([\n",
    "                ('fc1',   nn.Sequential(\n",
    "                                        nn.Linear(512 * 7 * 7, 512),\n",
    "                                        nn.ReLU())),\n",
    "                ('fc2',   nn.Sequential(nn.Dropout(0.5),\n",
    "                                        nn.Linear(512, 43)\n",
    "                                        ))]))\n",
    "    def forward(self,x):\n",
    "        backbone_feat = self.backbone.features(x)\n",
    "        backbone_feat_BAM = self.BAM(backbone_feat)\n",
    "\n",
    "        backbone_feat_BAM = backbone_feat_BAM.view(-1, 512 * 7 * 7)\n",
    "        score = self.classifier(backbone_feat_BAM)\n",
    "        return backbone_feat_BAM, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b0d88ea-efa3-4477-8ddb-d70dba61a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BAMVGG16():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if (torch.cuda.is_available()):\n",
    "        print(f'Current device:{torch.cuda.current_device()}\\nName of device:{torch.cuda.get_device_name(0)}\\n')\n",
    "    print(\"using {} device.\".format(device))\n",
    "\n",
    "    data_transform = {\n",
    "        \"train\": transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "        \"validation\": transforms.Compose([transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}\n",
    "\n",
    "    image_path = './dataset/' \n",
    "    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"Train\"),\n",
    "                                         transform=data_transform[\"train\"])\n",
    "    train_num = len(train_dataset)\n",
    "\n",
    "    exp_list = train_dataset.class_to_idx\n",
    "\n",
    "    batch_size = 16\n",
    "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])\n",
    "    print('Using {} dataloader workers every process'.format(nw))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=nw)\n",
    "\n",
    "    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"Test\"),\n",
    "                                            transform=data_transform[\"validation\"])\n",
    "    val_num = len(validate_dataset)\n",
    "    validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "                                                  batch_size=batch_size, shuffle=False,\n",
    "                                                  num_workers=nw)\n",
    "    print(\"using {} images for training, {} images for validation.\".format(train_num,\n",
    "                                                                           val_num))\n",
    "    \n",
    "    net = BAM_VGG16()\n",
    "    net.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()  #交叉熵损失函数\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "    epochs = 10\n",
    "    best_acc = 0.0\n",
    "    save_path = './{}Net.pth'.format('BAM_VGG')\n",
    "    train_steps = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(train_loader)\n",
    "        \n",
    "        confusion_matrix = torch.zeros((43,43),dtype=torch.int)\n",
    "        for step, data in enumerate(train_bar):\n",
    "            images, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            feat,outputs = net(images.to(device))\n",
    "\n",
    "            pred = torch.argmax(outputs.clone().detach().cpu(),dim=1)\n",
    "\n",
    "            for cur_pred, cur_label in zip(pred, labels):\n",
    "                confusion_matrix[cur_pred, cur_label] += 1\n",
    "            \n",
    "            # print(f'{outputs}\\n')    \n",
    "            loss = loss_function(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,\n",
    "                                                                     epochs,\n",
    "                                                                     loss)\n",
    "        # print(confusion_matrix)\n",
    "\n",
    "        # validate\n",
    "        net.eval()\n",
    "        acc = 0.0 \n",
    "        with torch.no_grad():  #验证的过程不计算损失梯度\n",
    "            val_bar = tqdm(validate_loader)\n",
    "            for val_data in val_bar:\n",
    "                val_images, val_labels = val_data\n",
    "                feat,outputs = net(val_images.to(device))\n",
    "                predict_y = torch.max(outputs, dim=1)[1]\n",
    "                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
    "\n",
    "        val_accurate = acc / val_num\n",
    "        print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
    "              (epoch + 1, running_loss / train_steps, val_accurate))\n",
    "\n",
    "        if val_accurate > best_acc:\n",
    "            best_acc = val_accurate\n",
    "            torch.save(net.state_dict(), save_path)\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e303d804-6e78-4690-b2dc-0c930ab660c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device:0\n",
      "Name of device:NVIDIA TITAN RTX\n",
      "\n",
      "using cuda:0 device.\n",
      "Using 8 dataloader workers every process\n",
      "using 39209 images for training, 12630 images for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[1/10] loss:0.014: 100%|██████████| 2451/2451 [15:51<00:00,  2.58it/s]\n",
      "100%|██████████| 790/790 [01:47<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] train_loss: 0.226  val_accuracy: 0.954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[2/10] loss:0.001: 100%|██████████| 2451/2451 [15:44<00:00,  2.59it/s]\n",
      "100%|██████████| 790/790 [01:47<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] train_loss: 0.059  val_accuracy: 0.962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[3/10] loss:0.001: 100%|██████████| 2451/2451 [16:05<00:00,  2.54it/s]\n",
      "100%|██████████| 790/790 [01:40<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3] train_loss: 0.036  val_accuracy: 0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[4/10] loss:0.000: 100%|██████████| 2451/2451 [14:20<00:00,  2.85it/s]\n",
      "100%|██████████| 790/790 [01:33<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4] train_loss: 0.030  val_accuracy: 0.971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[5/10] loss:0.002: 100%|██████████| 2451/2451 [13:03<00:00,  3.13it/s]\n",
      "100%|██████████| 790/790 [01:24<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 5] train_loss: 0.026  val_accuracy: 0.962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[6/10] loss:0.000: 100%|██████████| 2451/2451 [12:40<00:00,  3.22it/s]\n",
      "100%|██████████| 790/790 [01:28<00:00,  8.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6] train_loss: 0.020  val_accuracy: 0.972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[7/10] loss:0.004: 100%|██████████| 2451/2451 [14:15<00:00,  2.87it/s]\n",
      "100%|██████████| 790/790 [01:35<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 7] train_loss: 0.021  val_accuracy: 0.965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[8/10] loss:0.000: 100%|██████████| 2451/2451 [14:31<00:00,  2.81it/s]\n",
      "100%|██████████| 790/790 [01:40<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 8] train_loss: 0.017  val_accuracy: 0.963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[9/10] loss:0.000: 100%|██████████| 2451/2451 [14:50<00:00,  2.75it/s]\n",
      "100%|██████████| 790/790 [01:41<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 9] train_loss: 0.014  val_accuracy: 0.967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[10/10] loss:0.000: 100%|██████████| 2451/2451 [15:23<00:00,  2.65it/s]\n",
      "100%|██████████| 790/790 [01:49<00:00,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 10] train_loss: 0.017  val_accuracy: 0.968\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BAMVGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "868bfcbc-3216-4667-9d5d-63789c9f231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/asdf2kr/BAM-CBAM-pytorch/tree/master/Models\n",
    "\n",
    "def conv1x1(in_channels, out_channels, stride=1):\n",
    "    ''' 1x1 convolution '''\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride=1, padding=1, dilation=1):\n",
    "    ''' 3x3 convolution '''\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding, dilation=dilation, bias=False)\n",
    "\n",
    "def conv7x7(in_channels, out_channels, stride=1, padding=3, dilation=1):\n",
    "    ''' 7x7 convolution '''\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=7, stride=stride, padding=padding, dilation=dilation, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75ed4216-2c58-4957-b015-8fc6032878cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BAM(nn.Module):\n",
    "    def __init__(self, in_channel, reduction_ratio, dilation):\n",
    "        super(BAM, self).__init__()\n",
    "        self.hid_channel = in_channel // reduction_ratio\n",
    "        self.dilation = dilation\n",
    "        self.globalAvgPool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=in_channel, out_features=self.hid_channel)\n",
    "        self.bn1_1d = nn.BatchNorm1d(self.hid_channel)\n",
    "        self.fc2 = nn.Linear(in_features=self.hid_channel, out_features=in_channel)\n",
    "        self.bn2_1d = nn.BatchNorm1d(in_channel)\n",
    "\n",
    "        self.conv1 = conv1x1(in_channel, self.hid_channel)\n",
    "        self.bn1_2d = nn.BatchNorm2d(self.hid_channel)\n",
    "        self.conv2 = conv3x3(self.hid_channel, self.hid_channel, stride=1, padding=self.dilation, dilation=self.dilation)\n",
    "        self.bn2_2d = nn.BatchNorm2d(self.hid_channel)\n",
    "        self.conv3 = conv3x3(self.hid_channel, self.hid_channel, stride=1, padding=self.dilation, dilation=self.dilation)\n",
    "        self.bn3_2d = nn.BatchNorm2d(self.hid_channel)\n",
    "        self.conv4 = conv1x1(self.hid_channel, 1)\n",
    "        self.bn4_2d = nn.BatchNorm2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel attention\n",
    "        Mc = self.globalAvgPool(x)\n",
    "        Mc = Mc.view(Mc.size(0), -1)\n",
    "\n",
    "        Mc = self.fc1(Mc)\n",
    "        Mc = self.bn1_1d(Mc)\n",
    "        Mc = self.relu(Mc)\n",
    "\n",
    "        Mc = self.fc2(Mc)\n",
    "        Mc = self.bn2_1d(Mc)\n",
    "        Mc = self.relu(Mc)\n",
    "\n",
    "        Mc = Mc.view(Mc.size(0), Mc.size(1), 1, 1)\n",
    "\n",
    "        # Spatial attention\n",
    "        Ms = self.conv1(x)\n",
    "        Ms = self.bn1_2d(Ms)\n",
    "        Ms = self.relu(Ms)\n",
    "\n",
    "        Ms = self.conv2(Ms)\n",
    "        Ms = self.bn2_2d(Ms)\n",
    "        Ms = self.relu(Ms)\n",
    "\n",
    "        Ms = self.conv3(Ms)\n",
    "        Ms = self.bn3_2d(Ms)\n",
    "        Ms = self.relu(Ms)\n",
    "\n",
    "        Ms = self.conv4(Ms)\n",
    "        Ms = self.bn4_2d(Ms)\n",
    "        Ms = self.relu(Ms)\n",
    "\n",
    "        Ms = Ms.view(x.size(0), 1, x.size(2), x.size(3))\n",
    "        Mf = 1 + self.sigmoid(Mc * Ms)\n",
    "        return x * Mf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d522664-93fe-4ddb-bc37-7a4c9e97e583",
   "metadata": {},
   "source": [
    "https://github.com/asdf2kr/BAM-CBAM-pytorch/tree/master/Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f1700e1-4701-4be0-895f-74ce5df6437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, hid_channels, atte='bam', ratio=16, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, hid_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(hid_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(hid_channels, hid_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(hid_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "        if atte == 'cbam':\n",
    "            self.atte = CBAM(hid_channels, ratio)\n",
    "        else:\n",
    "            self.atte = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        # CBAM\n",
    "        if not self.atte is None:\n",
    "            out = self.atte(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class BottleneckBlock(nn.Module): # bottelneck-block, over the 50 layers.\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, hid_channels, atte='bam', ratio=16, stride=1, downsample=None):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        self.downsample = downsample\n",
    "        out_channels = hid_channels * self.expansion\n",
    "        self.conv1 = conv1x1(in_channels, hid_channels)\n",
    "        self.bn1 = nn.BatchNorm2d(hid_channels)\n",
    "\n",
    "        self.conv2 = conv3x3(hid_channels, hid_channels, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(hid_channels)\n",
    "\n",
    "        self.conv3 = conv1x1(hid_channels, out_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if atte == 'cbam':\n",
    "            self.atte = CBAM(out_channels, ratio)\n",
    "        else:\n",
    "            self.atte = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x # indentity\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        if not self.atte is None:\n",
    "            out = self.atte(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    '''\n",
    "    *50-layer\n",
    "        conv1 (output: 112x112)\n",
    "            7x7, 64, stride 2\n",
    "        conv2 (output: 56x56)\n",
    "            3x3 max pool, stride 2\n",
    "            [ 1x1, 64  ]\n",
    "            [ 3x3, 64  ] x 3\n",
    "            [ 1x1, 256 ]\n",
    "        cov3 (output: 28x28)\n",
    "            [ 1x1, 128 ]\n",
    "            [ 3x3, 128 ] x 4\n",
    "            [ 1x1, 512 ]\n",
    "        cov4 (output: 14x14)\n",
    "            [ 1x1, 256 ]\n",
    "            [ 3x3, 256 ] x 6\n",
    "            [ 1x1, 1024]\n",
    "        cov5 (output: 28x28)\n",
    "            [ 1x1, 512 ]\n",
    "            [ 3x3, 512 ] x 3\n",
    "            [ 1x1, 2048]\n",
    "        _ (output: 1x1)\n",
    "            average pool, 100-d fc, softmax\n",
    "        FLOPs 3.8x10^9\n",
    "    '''\n",
    "    '''\n",
    "    *101-layer\n",
    "        conv1 (output: 112x112)\n",
    "            7x7, 64, stride 2\n",
    "        conv2 (output: 56x56)\n",
    "            3x3 max pool, stride 2\n",
    "            [ 1x1, 64  ]\n",
    "            [ 3x3, 64  ] x 3\n",
    "            [ 1x1, 256 ]\n",
    "        cov3 (output: 28x28)\n",
    "            [ 1x1, 128 ]\n",
    "            [ 3x3, 128 ] x 4\n",
    "            [ 1x1, 512 ]\n",
    "        cov4 (output: 14x14)\n",
    "            [ 1x1, 256 ]\n",
    "            [ 3x3, 256 ] x 23\n",
    "            [ 1x1, 1024]\n",
    "        cov5 (output: 28x28)\n",
    "            [ 1x1, 512 ]\n",
    "            [ 3x3, 512 ] x 3\n",
    "            [ 1x1, 2048]\n",
    "        _ (output: 1x1)\n",
    "            average pool, 100-d fc, softmax\n",
    "        FLOPs 7.6x10^9\n",
    "    '''\n",
    "    def __init__(self, block, layers, num_classes=1000, atte='bam', ratio=16, dilation=4):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "        self.layers = layers\n",
    "        self.in_channels = 64\n",
    "        self.atte = atte\n",
    "        self.ratio = ratio\n",
    "        self.dilation = dilation\n",
    "\n",
    "        if num_classes == 1000:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            )\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        if self.atte == 'bam':\n",
    "            self.bam1 = BAM(64*block.expansion, self.ratio, self.dilation)\n",
    "            self.bam2 = BAM(128*block.expansion, self.ratio, self.dilation)\n",
    "            self.bam3 = BAM(256*block.expansion, self.ratio, self.dilation)\n",
    "\n",
    "        self.conv2 = self.get_layers(block, 64, self.layers[0])\n",
    "        self.conv3 = self.get_layers(block, 128, self.layers[1], stride=2)\n",
    "        self.conv4 = self.get_layers(block, 256, self.layers[2], stride=2)\n",
    "        self.conv5 = self.get_layers(block, 512, self.layers[3], stride=2)\n",
    "        self.avgPool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.fc.weight)\n",
    "        for m in self.state_dict():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        torch.nn.init.kaiming_normal_(self.fc.weight)\n",
    "        for m in self.state_dict():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def get_layers(self, block, hid_channels, n_layers, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != hid_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                    conv1x1(self.in_channels, hid_channels * block.expansion, stride),\n",
    "                    nn.BatchNorm2d(hid_channels * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, hid_channels, self.atte, self.ratio, stride, downsample))\n",
    "        self.in_channels = hid_channels * block.expansion\n",
    "\n",
    "        for _ in range(1, n_layers):\n",
    "            layers.append(block(self.in_channels, hid_channels, self.atte, self.ratio))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            Example tensor shape based on resnet101\n",
    "        '''\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        if self.atte == 'bam':\n",
    "            x = self.bam1(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        if self.atte == 'bam':\n",
    "            x = self.bam2(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        if self.atte == 'bam':\n",
    "            x = self.bam3(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.avgPool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "def BAMresnet50(**kwargs):\n",
    "    return ResNet(BottleneckBlock, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "def resnet101(**kwargs):\n",
    "    ''' ResNet-101 Model'''\n",
    "    return ResNet(BottleneckBlock, [3, 4, 23, 3], **kwargs)\n",
    "\n",
    "def resnet152(**kwargs):\n",
    "    return ResNet(BottleneckBlock, [3, 8, 36, 3], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835cf37f-0663-4b0e-a77b-c26e28ef79c4",
   "metadata": {},
   "source": [
    "## ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e69bf66-2506-49de-9cc2-2c2fc8be1926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resnet50():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if (torch.cuda.is_available()):\n",
    "        print(f'Current device:{torch.cuda.current_device()}\\nName of device:{torch.cuda.get_device_name(0)}\\n')\n",
    "    print(\"using {} device.\".format(device))\n",
    "\n",
    "    data_transform = {\n",
    "        \"train\": transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "        \"validation\": transforms.Compose([transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}\n",
    "\n",
    "    image_path = './dataset/' \n",
    "    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"Train\"),\n",
    "                                         transform=data_transform[\"train\"])\n",
    "    train_num = len(train_dataset)\n",
    "\n",
    "    exp_list = train_dataset.class_to_idx\n",
    "\n",
    "    batch_size = 16\n",
    "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])\n",
    "    print('Using {} dataloader workers every process'.format(nw))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=nw)\n",
    "\n",
    "    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"Test\"),\n",
    "                                            transform=data_transform[\"validation\"])\n",
    "    val_num = len(validate_dataset)\n",
    "    validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "                                                  batch_size=batch_size, shuffle=False,\n",
    "                                                  num_workers=nw)\n",
    "    print(\"using {} images for training, {} images for validation.\".format(train_num,\n",
    "                                                                           val_num))\n",
    "    \n",
    "    net = resnet50()\n",
    "    net.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()  #交叉熵损失函数\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "    epochs = 10\n",
    "    best_acc = 0.0\n",
    "    save_path = './{}Net.pth'.format('Res')\n",
    "    train_steps = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(train_loader)\n",
    "        \n",
    "        # confusion_matrix = torch.zeros((43,43),dtype=torch.int)\n",
    "        for step, data in enumerate(train_bar):\n",
    "            images, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images.to(device))\n",
    "\n",
    "            pred = torch.argmax(outputs.clone().detach().cpu(),dim=1)\n",
    "\n",
    "            #for cur_pred, cur_label in zip(pred, labels):\n",
    "                # confusion_matrix[cur_pred, cur_label] += 1\n",
    "                \n",
    "            loss = loss_function(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,\n",
    "                                                                     epochs,\n",
    "                                                                     loss)\n",
    "        # print(confusion_matrix)\n",
    "\n",
    "        # validate\n",
    "        net.eval()\n",
    "        acc = 0.0 \n",
    "        with torch.no_grad():  #验证的过程不计算损失梯度\n",
    "            val_bar = tqdm(validate_loader)\n",
    "            for val_data in val_bar:\n",
    "                val_images, val_labels = val_data\n",
    "                outputs = net(val_images.to(device))\n",
    "                predict_y = torch.max(outputs, dim=1)[1]\n",
    "                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
    "\n",
    "        val_accurate = acc / val_num\n",
    "        print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
    "              (epoch + 1, running_loss / train_steps, val_accurate))\n",
    "\n",
    "        if val_accurate > best_acc:\n",
    "            best_acc = val_accurate\n",
    "            torch.save(net.state_dict(), save_path)\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f658940-68aa-4779-8ca9-2af29c8099a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device:0\n",
      "Name of device:NVIDIA TITAN RTX\n",
      "\n",
      "using cuda:0 device.\n",
      "Using 8 dataloader workers every process\n",
      "using 39209 images for training, 12630 images for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[1/10] loss:0.003: 100%|██████████| 2451/2451 [05:32<00:00,  7.38it/s]\n",
      "100%|██████████| 790/790 [00:35<00:00, 22.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] train_loss: 0.754  val_accuracy: 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[2/10] loss:0.014: 100%|██████████| 2451/2451 [05:58<00:00,  6.84it/s]\n",
      "100%|██████████| 790/790 [00:36<00:00, 21.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] train_loss: 0.119  val_accuracy: 0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[3/10] loss:0.003: 100%|██████████| 2451/2451 [06:02<00:00,  6.76it/s]\n",
      "100%|██████████| 790/790 [00:36<00:00, 21.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3] train_loss: 0.071  val_accuracy: 0.917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[4/10] loss:0.006: 100%|██████████| 2451/2451 [05:43<00:00,  7.14it/s]\n",
      "100%|██████████| 790/790 [00:34<00:00, 23.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4] train_loss: 0.052  val_accuracy: 0.956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[5/10] loss:0.002: 100%|██████████| 2451/2451 [05:36<00:00,  7.28it/s]\n",
      "100%|██████████| 790/790 [00:33<00:00, 23.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 5] train_loss: 0.036  val_accuracy: 0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[6/10] loss:0.024: 100%|██████████| 2451/2451 [05:37<00:00,  7.25it/s]\n",
      "100%|██████████| 790/790 [00:33<00:00, 23.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6] train_loss: 0.033  val_accuracy: 0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[7/10] loss:0.273: 100%|██████████| 2451/2451 [05:36<00:00,  7.27it/s]\n",
      "100%|██████████| 790/790 [00:33<00:00, 23.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 7] train_loss: 0.024  val_accuracy: 0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[8/10] loss:0.001: 100%|██████████| 2451/2451 [05:38<00:00,  7.25it/s]\n",
      "100%|██████████| 790/790 [00:34<00:00, 23.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 8] train_loss: 0.021  val_accuracy: 0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[9/10] loss:0.000: 100%|██████████| 2451/2451 [05:36<00:00,  7.29it/s]\n",
      "100%|██████████| 790/790 [00:33<00:00, 23.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 9] train_loss: 0.019  val_accuracy: 0.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[10/10] loss:0.009: 100%|██████████| 2451/2451 [05:35<00:00,  7.30it/s]\n",
      "100%|██████████| 790/790 [00:33<00:00, 23.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 10] train_loss: 0.017  val_accuracy: 0.968\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Resnet50()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66647a37-d9fe-4a1f-b5e8-80969987f9da",
   "metadata": {},
   "source": [
    "## BAM + ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9556a49a-327d-4b7b-9f1b-786d7ef8ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BAM_Resnet50():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if (torch.cuda.is_available()):\n",
    "        print(f'Current device:{torch.cuda.current_device()}\\nName of device:{torch.cuda.get_device_name(0)}\\n')\n",
    "    print(\"using {} device.\".format(device))\n",
    "\n",
    "    data_transform = {\n",
    "        \"train\": transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "        \"validation\": transforms.Compose([transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}\n",
    "\n",
    "    image_path = './dataset/' \n",
    "    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"Train\"),\n",
    "                                         transform=data_transform[\"train\"])\n",
    "    train_num = len(train_dataset)\n",
    "\n",
    "    exp_list = train_dataset.class_to_idx\n",
    "\n",
    "    batch_size = 16\n",
    "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])\n",
    "    print('Using {} dataloader workers every process'.format(nw))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=nw)\n",
    "\n",
    "    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"Test\"),\n",
    "                                            transform=data_transform[\"validation\"])\n",
    "    val_num = len(validate_dataset)\n",
    "    validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "                                                  batch_size=batch_size, shuffle=False,\n",
    "                                                  num_workers=nw)\n",
    "    print(\"using {} images for training, {} images for validation.\".format(train_num,\n",
    "                                                                           val_num))\n",
    "    \n",
    "    net = BAMresnet50()\n",
    "    net.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()  #交叉熵损失函数\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "    epochs = 10\n",
    "    best_acc = 0.0\n",
    "    save_path = './{}Net.pth'.format('BAM_Res')\n",
    "    train_steps = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(train_loader)\n",
    "        \n",
    "        # confusion_matrix = torch.zeros((43,43),dtype=torch.int)\n",
    "        for step, data in enumerate(train_bar):\n",
    "            images, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images.to(device))\n",
    "\n",
    "            pred = torch.argmax(outputs.clone().detach().cpu(),dim=1)\n",
    "\n",
    "            #for cur_pred, cur_label in zip(pred, labels):\n",
    "                # confusion_matrix[cur_pred, cur_label] += 1\n",
    "                \n",
    "            loss = loss_function(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,\n",
    "                                                                     epochs,\n",
    "                                                                     loss)\n",
    "        # print(confusion_matrix)\n",
    "\n",
    "        # validate\n",
    "        net.eval()\n",
    "        acc = 0.0 \n",
    "        with torch.no_grad():  #验证的过程不计算损失梯度\n",
    "            val_bar = tqdm(validate_loader)\n",
    "            for val_data in val_bar:\n",
    "                val_images, val_labels = val_data\n",
    "                outputs = net(val_images.to(device))\n",
    "                predict_y = torch.max(outputs, dim=1)[1]\n",
    "                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
    "\n",
    "        val_accurate = acc / val_num\n",
    "        print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
    "              (epoch + 1, running_loss / train_steps, val_accurate))\n",
    "\n",
    "        if val_accurate > best_acc:\n",
    "            best_acc = val_accurate\n",
    "            torch.save(net.state_dict(), save_path)\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba330b6b-4050-4859-a27c-5a90fad4d07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device:0\n",
      "Name of device:NVIDIA TITAN RTX\n",
      "\n",
      "using cuda:0 device.\n",
      "Using 8 dataloader workers every process\n",
      "using 39209 images for training, 12630 images for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[1/10] loss:0.260: 100%|██████████| 2451/2451 [12:02<00:00,  3.39it/s]\n",
      "100%|██████████| 790/790 [01:02<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] train_loss: 1.085  val_accuracy: 0.851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[2/10] loss:0.007: 100%|██████████| 2451/2451 [11:44<00:00,  3.48it/s]\n",
      "100%|██████████| 790/790 [01:02<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] train_loss: 0.144  val_accuracy: 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[3/10] loss:0.001: 100%|██████████| 2451/2451 [11:41<00:00,  3.50it/s]\n",
      "100%|██████████| 790/790 [01:02<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3] train_loss: 0.092  val_accuracy: 0.849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[4/10] loss:0.545: 100%|██████████| 2451/2451 [11:44<00:00,  3.48it/s]\n",
      "100%|██████████| 790/790 [01:03<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4] train_loss: 0.066  val_accuracy: 0.867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[5/10] loss:0.001: 100%|██████████| 2451/2451 [11:44<00:00,  3.48it/s]\n",
      "100%|██████████| 790/790 [01:03<00:00, 12.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 5] train_loss: 0.057  val_accuracy: 0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[6/10] loss:0.000: 100%|██████████| 2451/2451 [11:48<00:00,  3.46it/s]\n",
      "100%|██████████| 790/790 [01:03<00:00, 12.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6] train_loss: 0.044  val_accuracy: 0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[7/10] loss:0.009: 100%|██████████| 2451/2451 [11:44<00:00,  3.48it/s]\n",
      "100%|██████████| 790/790 [01:03<00:00, 12.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 7] train_loss: 0.037  val_accuracy: 0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[8/10] loss:0.006: 100%|██████████| 2451/2451 [11:51<00:00,  3.45it/s]\n",
      "100%|██████████| 790/790 [01:03<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 8] train_loss: 0.033  val_accuracy: 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[9/10] loss:0.002: 100%|██████████| 2451/2451 [11:47<00:00,  3.46it/s]\n",
      "100%|██████████| 790/790 [01:03<00:00, 12.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 9] train_loss: 0.026  val_accuracy: 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[10/10] loss:0.003: 100%|██████████| 2451/2451 [11:44<00:00,  3.48it/s]\n",
      "100%|██████████| 790/790 [01:03<00:00, 12.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 10] train_loss: 0.027  val_accuracy: 0.948\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BAM_Resnet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2d139c3-4cc3-44f9-964b-7ad82916452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.backbone = models.alexnet(pretrained=True)\n",
    "        # self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.classifier = nn.Sequential(OrderedDict([\n",
    "                ('fc1',   nn.Sequential(\n",
    "                                        nn.Linear(256 * 6 * 6, 512),\n",
    "                                        nn.ReLU())),\n",
    "                ('fc2',   nn.Sequential(nn.Dropout(0.5),\n",
    "                                        nn.Linear(512, 43)\n",
    "                                        ))]))\n",
    "    def forward(self,x):        \n",
    "        backbone_feat = self.backbone.features(x)\n",
    "        backbone_feat = backbone_feat.view(-1, 256 * 6 * 6)\n",
    "        score = self.classifier(backbone_feat)\n",
    "        return backbone_feat, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "378bb4e3-7913-44cd-a397-ee086dc6d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Alex_Net():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if (torch.cuda.is_available()):\n",
    "        print(f'Current device:{torch.cuda.current_device()}\\nName of device:{torch.cuda.get_device_name(0)}\\n')\n",
    "    print(\"using {} device.\".format(device))\n",
    "\n",
    "    data_transform = {\n",
    "        \"train\": transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "        \"validation\": transforms.Compose([transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}\n",
    "\n",
    "    image_path = './dataset/'  \n",
    "    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"Train\"),\n",
    "                                         transform=data_transform[\"train\"])\n",
    "    train_num = len(train_dataset)\n",
    "\n",
    "    exp_list = train_dataset.class_to_idx\n",
    "\n",
    "    batch_size = 16\n",
    "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])\n",
    "    print('Using {} dataloader workers every process'.format(nw))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=nw)\n",
    "\n",
    "    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"Test\"),\n",
    "                                            transform=data_transform[\"validation\"])\n",
    "    val_num = len(validate_dataset)\n",
    "    validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "                                                  batch_size=batch_size, shuffle=False,\n",
    "                                                  num_workers=nw)\n",
    "    print(\"using {} images for training, {} images for validation.\".format(train_num,\n",
    "                                                                           val_num))\n",
    "    \n",
    "    net = AlexNet()\n",
    "    net.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()  #交叉熵损失函数\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "    epochs = 10\n",
    "    best_acc = 0.0\n",
    "    save_path = './{}Net.pth'.format('Alex')\n",
    "    train_steps = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(train_loader)\n",
    "        \n",
    "        confusion_matrix = torch.zeros((43,43),dtype=torch.int)\n",
    "        for step, data in enumerate(train_bar):\n",
    "            images, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            feat,outputs = net(images.to(device))\n",
    "\n",
    "            pred = torch.argmax(outputs.clone().detach().cpu(),dim=1)\n",
    "\n",
    "            for cur_pred, cur_label in zip(pred, labels):\n",
    "                confusion_matrix[cur_pred, cur_label] += 1\n",
    "                \n",
    "            loss = loss_function(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,\n",
    "                                                                     epochs,\n",
    "                                                                     loss)\n",
    "        # print(confusion_matrix)\n",
    "\n",
    "        # validate\n",
    "        net.eval()\n",
    "        acc = 0.0 \n",
    "        with torch.no_grad():  #验证的过程不计算损失梯度\n",
    "            val_bar = tqdm(validate_loader)\n",
    "            for val_data in val_bar:\n",
    "                val_images, val_labels = val_data\n",
    "                feat,outputs = net(val_images.to(device))\n",
    "                predict_y = torch.max(outputs, dim=1)[1]\n",
    "                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
    "\n",
    "        val_accurate = acc / val_num\n",
    "        print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
    "              (epoch + 1, running_loss / train_steps, val_accurate))\n",
    "\n",
    "        if val_accurate > best_acc:\n",
    "            best_acc = val_accurate\n",
    "            torch.save(net.state_dict(), save_path)\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1c24531-bdb6-4abd-9e91-4331ae4d47ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device:0\n",
      "Name of device:NVIDIA TITAN RTX\n",
      "\n",
      "using cuda:0 device.\n",
      "Using 8 dataloader workers every process\n",
      "using 39209 images for training, 12630 images for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[1/10] loss:0.134: 100%|██████████| 2451/2451 [00:51<00:00, 47.56it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 76.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] train_loss: 0.341  val_accuracy: 0.941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[2/10] loss:0.004: 100%|██████████| 2451/2451 [00:49<00:00, 49.20it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 76.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] train_loss: 0.056  val_accuracy: 0.945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[3/10] loss:0.001: 100%|██████████| 2451/2451 [00:50<00:00, 48.21it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 76.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3] train_loss: 0.037  val_accuracy: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[4/10] loss:0.001: 100%|██████████| 2451/2451 [00:50<00:00, 48.39it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 76.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4] train_loss: 0.028  val_accuracy: 0.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[5/10] loss:0.001: 100%|██████████| 2451/2451 [00:50<00:00, 48.73it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 75.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 5] train_loss: 0.021  val_accuracy: 0.962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[6/10] loss:0.002: 100%|██████████| 2451/2451 [00:50<00:00, 48.74it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 76.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6] train_loss: 0.018  val_accuracy: 0.959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[7/10] loss:0.000: 100%|██████████| 2451/2451 [00:50<00:00, 48.90it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 76.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 7] train_loss: 0.015  val_accuracy: 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[8/10] loss:0.000: 100%|██████████| 2451/2451 [00:49<00:00, 49.10it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 76.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 8] train_loss: 0.016  val_accuracy: 0.954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[9/10] loss:0.000: 100%|██████████| 2451/2451 [00:49<00:00, 49.25it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 75.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 9] train_loss: 0.014  val_accuracy: 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[10/10] loss:0.005: 100%|██████████| 2451/2451 [00:49<00:00, 49.45it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 76.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 10] train_loss: 0.013  val_accuracy: 0.959\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Alex_Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b698c9bd-eb00-4271-9c2b-a71477ae6615",
   "metadata": {},
   "source": [
    "## BAM + AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fc4a2eb-a7bb-49e3-a0cd-9385321bb7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BAM_alexnet(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(BAM_alexnet, self).__init__()\n",
    "        self.backbone = models.alexnet(pretrained=True)\n",
    "        self.BAM = BAMBlock(channel=256,dia_val=1)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.classifier = nn.Sequential(OrderedDict([\n",
    "                ('fc1',   nn.Sequential(\n",
    "                                        nn.Linear(256 * 6 * 6, 512),\n",
    "                                        nn.ReLU())),\n",
    "                ('fc2',   nn.Sequential(nn.Dropout(0.5),\n",
    "                                        nn.Linear(512, 43)\n",
    "                                        ))]))\n",
    "    def forward(self,x):\n",
    "        backbone_feat = self.backbone.features(x)\n",
    "        backbone_feat_BAM = self.BAM(backbone_feat)\n",
    "\n",
    "        backbone_feat_BAM = backbone_feat_BAM.view(-1, 256 * 6 * 6)\n",
    "        score = self.classifier(backbone_feat_BAM)\n",
    "        return backbone_feat_BAM, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24e04cf4-aaf1-46c8-a2cd-8d3d679c9287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BAMAlexNet():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if (torch.cuda.is_available()):\n",
    "        print(f'Current device:{torch.cuda.current_device()}\\nName of device:{torch.cuda.get_device_name(0)}\\n')\n",
    "    print(\"using {} device.\".format(device))\n",
    "    \n",
    "    data_transform = {\n",
    "        \"train\": transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "        \"validation\": transforms.Compose([transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}\n",
    "\n",
    "    image_path = './dataset/' \n",
    "    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"Train\"),\n",
    "                                         transform=data_transform[\"train\"])\n",
    "    train_num = len(train_dataset)\n",
    "\n",
    "    exp_list = train_dataset.class_to_idx\n",
    "\n",
    "    batch_size = 16\n",
    "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])\n",
    "    print('Using {} dataloader workers every process'.format(nw))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=nw)\n",
    "\n",
    "    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"Test\"),\n",
    "                                            transform=data_transform[\"validation\"])\n",
    "    val_num = len(validate_dataset)\n",
    "    validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "                                                  batch_size=batch_size, shuffle=False,\n",
    "                                                  num_workers=nw)\n",
    "    print(\"using {} images for training, {} images for validation.\".format(train_num,\n",
    "                                                                           val_num))\n",
    "    \n",
    "    net = BAM_alexnet()\n",
    "    net.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()  #交叉熵损失函数\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "    epochs = 10\n",
    "    best_acc = 0.0\n",
    "    save_path = './{}Net.pth'.format('BAM_Alex')\n",
    "    train_steps = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(train_loader)\n",
    "        \n",
    "        confusion_matrix = torch.zeros((43,43),dtype=torch.int)\n",
    "        for step, data in enumerate(train_bar):\n",
    "            images, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            feat,outputs = net(images.to(device))\n",
    "\n",
    "            pred = torch.argmax(outputs.clone().detach().cpu(),dim=1)\n",
    "\n",
    "            for cur_pred, cur_label in zip(pred, labels):\n",
    "                confusion_matrix[cur_pred, cur_label] += 1\n",
    "                \n",
    "            loss = loss_function(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,\n",
    "                                                                     epochs,\n",
    "                                                                     loss)\n",
    "        # print(confusion_matrix)\n",
    "\n",
    "        # validate\n",
    "        net.eval()\n",
    "        acc = 0.0 \n",
    "        with torch.no_grad():  #验证的过程不计算损失梯度\n",
    "            val_bar = tqdm(validate_loader)\n",
    "            for val_data in val_bar:\n",
    "                val_images, val_labels = val_data\n",
    "                feat,outputs = net(val_images.to(device))\n",
    "                predict_y = torch.max(outputs, dim=1)[1]\n",
    "                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
    "\n",
    "        val_accurate = acc / val_num\n",
    "        print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
    "              (epoch + 1, running_loss / train_steps, val_accurate))\n",
    "\n",
    "        if val_accurate > best_acc:\n",
    "            best_acc = val_accurate\n",
    "            torch.save(net.state_dict(), save_path)\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f29fd2f2-08ae-410c-b4ca-e94e8797e364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device:0\n",
      "Name of device:NVIDIA TITAN RTX\n",
      "\n",
      "using cuda:0 device.\n",
      "Using 8 dataloader workers every process\n",
      "using 39209 images for training, 12630 images for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[1/10] loss:0.063: 100%|██████████| 2451/2451 [00:56<00:00, 43.40it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 72.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] train_loss: 0.329  val_accuracy: 0.946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[2/10] loss:0.001: 100%|██████████| 2451/2451 [00:56<00:00, 43.42it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 72.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] train_loss: 0.061  val_accuracy: 0.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[3/10] loss:0.003: 100%|██████████| 2451/2451 [00:56<00:00, 43.47it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 72.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3] train_loss: 0.034  val_accuracy: 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[4/10] loss:0.008: 100%|██████████| 2451/2451 [00:56<00:00, 43.59it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 72.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4] train_loss: 0.028  val_accuracy: 0.956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[5/10] loss:0.000: 100%|██████████| 2451/2451 [00:56<00:00, 43.52it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 72.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 5] train_loss: 0.022  val_accuracy: 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[6/10] loss:0.133: 100%|██████████| 2451/2451 [00:56<00:00, 43.56it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 72.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6] train_loss: 0.020  val_accuracy: 0.923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[7/10] loss:0.000: 100%|██████████| 2451/2451 [00:56<00:00, 43.54it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 72.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 7] train_loss: 0.016  val_accuracy: 0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[8/10] loss:0.023: 100%|██████████| 2451/2451 [00:56<00:00, 43.32it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 72.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 8] train_loss: 0.016  val_accuracy: 0.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[9/10] loss:0.000: 100%|██████████| 2451/2451 [00:56<00:00, 43.58it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 71.88it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 9] train_loss: 0.014  val_accuracy: 0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[10/10] loss:0.001: 100%|██████████| 2451/2451 [00:56<00:00, 43.42it/s]\n",
      "100%|██████████| 790/790 [00:10<00:00, 71.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 10] train_loss: 0.012  val_accuracy: 0.961\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "BAMAlexNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0df171-d195-4c73-a5ed-409443e5970e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
